{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "# CA02: Email Spam Classifier using Naive Bayes\n",
        "\n",
        "## Assignment Overview\n",
        "This notebook implements a **supervised machine learning spam email classifier** using the **Naive Bayes algorithm**.\n",
        "\n",
        "### Objectives:\n",
        "1. **Train** a Naive Bayes model on 702 labeled emails (351 spam, 351 non-spam)\n",
        "2. **Test** the model on 260 new emails\n",
        "3. **Evaluate** the model's accuracy by comparing predictions with actual labels\n",
        "\n",
        "### Dataset Structure:\n",
        "- **Training Data**: `./train-mails/` - 702 emails for model training\n",
        "- **Test Data**: `./test-mails/` - 260 emails for model evaluation\n",
        "- **File Naming Convention**:\n",
        "  - Non-spam: `number-numbermsg[number].txt` (e.g., `3-1msg1.txt`)\n",
        "  - Spam: `spmsg[Number].txt` (e.g., `spmsga162.txt`)\n",
        "\n",
        "### Machine Learning Workflow:\n",
        "1. **Data Preparation**: Extract and clean email text\n",
        "2. **Feature Engineering**: Create word frequency matrix using top 3000 words\n",
        "3. **Model Training**: Train Gaussian Naive Bayes classifier\n",
        "4. **Prediction**: Classify test emails\n",
        "5. **Evaluation**: Calculate accuracy score\n",
        "\n",
        "---\n",
        "\n",
        "**IMPORTANT NOTE:**\n",
        "- The data folders must be located at `./train-mails` and `./test-mails`\n",
        "- This ensures code portability across different systems\n",
        "- Do not modify the original data files or folder names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMK92baiCdKb"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "We import the following libraries:\n",
        "- **os**: For file system operations (reading directories, file paths)\n",
        "- **numpy**: For numerical operations and matrix handling\n",
        "- **Counter**: From collections, to count word frequencies efficiently\n",
        "- **GaussianNB**: The Naive Bayes classifier from scikit-learn\n",
        "- **accuracy_score**: To evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Machine Learning imports from scikit-learn\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgwroBQCdKd"
      },
      "source": [
        "## Step 2: Create Dictionary Function\n",
        "\n",
        "### Purpose:\n",
        "This function creates a **dictionary of the 3000 most frequent words** from all training emails.\n",
        "\n",
        "### Why we need this:\n",
        "- Reduces dimensionality (instead of thousands of unique words, we use only 3000)\n",
        "- Removes noise by filtering out rare words and stop words\n",
        "- Creates a consistent feature space for the machine learning model\n",
        "\n",
        "### Process:\n",
        "1. Read all email files from the training directory\n",
        "2. Extract all words from all emails\n",
        "3. Count word frequencies using Counter\n",
        "4. Remove non-alphabetic words (numbers, punctuation, symbols)\n",
        "5. Remove single-character words (typically not meaningful)\n",
        "6. Return the 3000 most common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def make_Dictionary(root_dir):\n",
        "    \"\"\"\n",
        "    Creates a dictionary of the 3000 most frequent valid words from training emails.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    root_dir : str\n",
        "        Path to the directory containing training email files\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list of tuples\n",
        "        List of (word, frequency) tuples for the 3000 most common words\n",
        "    \"\"\"\n",
        "\n",
        "    all_words = []  # List to store all words from all emails\n",
        "\n",
        "    # Create list of full file paths for all files in the directory\n",
        "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "\n",
        "    # Iterate through each email file\n",
        "    for mail in emails:\n",
        "        with open(mail) as m:  # Open each email file\n",
        "            for line in m:  # Read line by line\n",
        "                words = line.split()  # Split line into words (whitespace delimiter)\n",
        "                all_words += words  # Add words to our master list\n",
        "\n",
        "    # Count frequency of each word using Counter (dictionary subclass)\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    # Create a copy of all words for safe iteration during deletion\n",
        "    list_to_remove = list(dictionary)\n",
        "\n",
        "    # Clean the dictionary by removing unwanted items\n",
        "    for item in list_to_remove:\n",
        "        # Remove words containing non-alphabetic characters (numbers, punctuation)\n",
        "        if item.isalpha() == False:\n",
        "            del dictionary[item]\n",
        "        # Remove single-character words (usually not meaningful)\n",
        "        elif len(item) == 1:\n",
        "            del dictionary[item]\n",
        "\n",
        "    # Get the 3000 most common words as a list of (word, count) tuples\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etYXxg0cCdKe"
      },
      "source": [
        "## Step 3: Feature Extraction Function\n",
        "\n",
        "### Purpose:\n",
        "This function converts emails into a **numerical feature matrix** that can be used by machine learning algorithms.\n",
        "\n",
        "### How it works:\n",
        "- Creates a matrix where:\n",
        "  - Each **row** represents one email\n",
        "  - Each **column** represents one word from our 3000-word dictionary\n",
        "  - Each **cell value** is the frequency of that word in that email\n",
        "\n",
        "### Process:\n",
        "1. Read all email files from the specified directory\n",
        "2. For each email:\n",
        "   - Extract words from line 3 (actual content, skipping subject and blank line)\n",
        "   - Count how many times each dictionary word appears\n",
        "   - Store counts in the feature matrix\n",
        "   - Label the email as spam (1) or non-spam (0) based on filename\n",
        "3. Return the feature matrix and labels\n",
        "\n",
        "### Example:\n",
        "If word \"free\" is at index 42 in dictionary and appears 3 times in email 5:\n",
        "- `features_matrix[5, 42] = 3`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def extract_features(mail_dir):\n",
        "    \"\"\"\n",
        "    Extracts features from emails and creates a numerical feature matrix.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    mail_dir : str\n",
        "        Path to directory containing email files\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (features_matrix, labels)\n",
        "        - features_matrix: numpy array of shape (n_emails, 3000) with word frequencies\n",
        "        - labels: numpy array of shape (n_emails,) with 0 for non-spam, 1 for spam\n",
        "    \"\"\"\n",
        "\n",
        "    # Get list of all file paths in the directory\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "\n",
        "    # Initialize feature matrix: rows = emails, columns = 3000 dictionary words\n",
        "    # All values start at 0 (word not present)\n",
        "    features_matrix = np.zeros((len(files), 3000))\n",
        "\n",
        "    # Initialize labels array: 0 = non-spam, 1 = spam\n",
        "    train_labels = np.zeros(len(files))\n",
        "\n",
        "    count = 1  # Counter for spam emails (for tracking purposes)\n",
        "    docID = 0  # Document ID (row index in feature matrix)\n",
        "\n",
        "    # Process each email file\n",
        "    for fil in files:\n",
        "        with open(fil) as fi:\n",
        "            # Enumerate through lines to identify line 3 (i=2, zero-indexed)\n",
        "            for i, line in enumerate(fi):\n",
        "                if i == 2:  # Line 3 contains the actual email content\n",
        "                    words = line.split()  # Split content into words\n",
        "\n",
        "                    # For each word in this email\n",
        "                    for word in words:\n",
        "                        wordID = 0  # Position of word in dictionary\n",
        "\n",
        "                        # Search for this word in our dictionary\n",
        "                        for i, d in enumerate(dictionary):\n",
        "                            if d[0] == word:  # d[0] is the word, d[1] is frequency\n",
        "                                wordID = i\n",
        "                                # Count occurrences and store in feature matrix\n",
        "                                features_matrix[docID, wordID] = words.count(word)\n",
        "\n",
        "        # Default label is 0 (non-spam)\n",
        "        train_labels[docID] = 0\n",
        "\n",
        "        # Parse filename to determine if email is spam\n",
        "        filepathTokens = fil.split('/')  # Split path by '/'\n",
        "        lastToken = filepathTokens[len(filepathTokens) - 1]  # Get filename\n",
        "\n",
        "        # If filename starts with \"spmsg\", it's a spam email\n",
        "        if lastToken.startswith(\"spmsg\"):\n",
        "            train_labels[docID] = 1  # Label as spam\n",
        "            count = count + 1  # Increment spam counter\n",
        "\n",
        "        docID = docID + 1  # Move to next email (next row)\n",
        "\n",
        "    return features_matrix, train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mh2Mwy7CdKf"
      },
      "source": [
        "## Step 4: Define Data Paths\n",
        "\n",
        "### Directory Structure:\n",
        "Set the paths to training and test data folders using **relative paths**.\n",
        "\n",
        "### Why relative paths?\n",
        "- Ensures code portability across different computers\n",
        "- Peer reviewers and instructors can run the code without modifications\n",
        "- Works regardless of the absolute location on the file system\n",
        "\n",
        "### Expected folder structure:\n",
        "```\n",
        ".\n",
        "├── CA02_NB_assignment.ipynb\n",
        "├── train-mails/\n",
        "│   ├── 1-1msg1.txt\n",
        "│   ├── spmsga1.txt\n",
        "│   └── ...\n",
        "└── test-mails/\n",
        "    ├── 1-1msg1.txt\n",
        "    ├── spmsga1.txt\n",
        "    └── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Define relative paths to data directories\n",
        "# './' means current directory where this notebook is located\n",
        "TRAIN_DIR = './train-mails'  # Training data: 702 emails\n",
        "TEST_DIR = './test-mails'    # Test data: 260 emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GGv-vndCdKh"
      },
      "source": [
        "## Step 5: Data Preparation and Feature Extraction\n",
        "\n",
        "### What happens in this step:\n",
        "\n",
        "1. **Create Dictionary**:\n",
        "   - Analyze all training emails\n",
        "   - Extract the 3000 most common valid words\n",
        "   - This becomes our feature set (vocabulary)\n",
        "\n",
        "2. **Extract Training Features**:\n",
        "   - Convert 702 training emails into a 702×3000 matrix\n",
        "   - Each cell contains word frequency\n",
        "   - Create corresponding labels (spam vs. non-spam)\n",
        "\n",
        "3. **Extract Test Features**:\n",
        "   - Convert 260 test emails into a 260×3000 matrix\n",
        "   - Use the SAME dictionary created from training data\n",
        "   - Create corresponding labels for evaluation\n",
        "\n",
        "### Why use the same dictionary for test data?\n",
        "- Machine learning models need consistent features\n",
        "- Test data must have the same structure as training data\n",
        "- Model was trained on these 3000 words, so it can only understand these words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "134lmhauyQxE",
        "outputId": "3c10a602-6cc3-4d02-830a-b72038cf4860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './train-mails'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4184581211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Build the dictionary from training emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This analyzes all training data and selects the 3000 most frequent valid words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_Dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reading and processing emails from TRAIN and TEST folders\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3365979852.py\u001b[0m in \u001b[0;36mmake_Dictionary\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Create list of full file paths for all files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0memails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Iterate through each email file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train-mails'"
          ]
        }
      ],
      "source": [
        "# Step 1: Build the dictionary from training emails\n",
        "# This analyzes all training data and selects the 3000 most frequent valid words\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "\n",
        "# Step 2: Extract features from training data\n",
        "# Converts training emails to numerical matrix and extracts labels\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "\n",
        "# Step 3: Extract features from test data\n",
        "# Uses the SAME dictionary to ensure consistency\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FS_BuqCdKh"
      },
      "source": [
        "## Step 6: Model Training, Prediction, and Evaluation\n",
        "\n",
        "### Machine Learning Pipeline:\n",
        "\n",
        "#### 1. **Model Training** (Gaussian Naive Bayes)\n",
        "   - **Algorithm**: Gaussian Naive Bayes assumes features follow a normal distribution\n",
        "   - **Training data**: 702 emails with their word frequency features\n",
        "   - **Learning process**: Model calculates probability distributions for each word in spam vs. non-spam emails\n",
        "   - **Why Naive Bayes?**\n",
        "     - Works well with text classification\n",
        "     - Fast training and prediction\n",
        "     - Handles high-dimensional data efficiently\n",
        "     - \"Naive\" assumption: treats each word as independent (simplification that works well in practice)\n",
        "\n",
        "#### 2. **Prediction**\n",
        "   - Apply trained model to test data (260 emails)\n",
        "   - Model calculates probability that each email is spam\n",
        "   - Assigns label: 1 (spam) or 0 (non-spam)\n",
        "\n",
        "#### 3. **Evaluation**\n",
        "   - Compare predicted labels with actual labels\n",
        "   - Calculate accuracy: (correct predictions) / (total predictions)\n",
        "   - Expected accuracy: ~96-97%\n",
        "\n",
        "### Mathematical Intuition:\n",
        "Naive Bayes calculates: P(Spam | Email) using Bayes' Theorem:\n",
        "- P(Spam | Email) = P(Email | Spam) × P(Spam) / P(Email)\n",
        "- Classifies as spam if P(Spam | Email) > P(Not Spam | Email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJjPwbgpCdKi"
      },
      "outputs": [],
      "source": [
        "# TRAINING PHASE\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "# Initialize the Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model on training features and labels\n",
        "# features_matrix: 702 emails × 3000 word frequencies\n",
        "# labels: 702 labels (0 or 1)\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "\n",
        "# PREDICTION PHASE\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "# Use the trained model to predict labels for test emails\n",
        "# Input: test_features_matrix (260 emails × 3000 word frequencies)\n",
        "# Output: predicted_labels (260 predictions: 0 or 1)\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "# EVALUATION PHASE\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "# Calculate accuracy: (number of correct predictions) / (total predictions)\n",
        "# Compares predicted_labels with actual test_labels\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "\n",
        "# Print the accuracy score (expected: ~0.965 or 96.5%)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "---\n",
        "\n",
        "## Results Interpretation\n",
        "\n",
        "### Expected Output:\n",
        "- **Accuracy Score**: ~0.9654 (96.54%)\n",
        "\n",
        "### What this means:\n",
        "- The model correctly classified approximately **251 out of 260** test emails\n",
        "- Only about **9 emails** were misclassified\n",
        "- This is excellent performance for a spam classifier!\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **High accuracy** indicates the Naive Bayes algorithm works well for email spam detection\n",
        "- The 3000-word feature space is sufficient to capture spam characteristics\n",
        "- Word frequency is a strong indicator of spam vs. legitimate emails\n",
        "\n",
        "### Potential Improvements:\n",
        "1. Use TF-IDF instead of raw word counts\n",
        "2. Include bigrams or trigrams (word pairs/triplets)\n",
        "3. Try other algorithms (SVM, Random Forest, Neural Networks)\n",
        "4. Perform cross-validation for more robust evaluation\n",
        "5. Add subject line as a separate feature\n",
        "\n",
        "---\n",
        "\n",
        "======================= END OF PROGRAM ========================="
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}